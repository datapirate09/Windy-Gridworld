{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUIFdvXavSlwn3Ctxg8hot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datapirate09/Windy-Gridworld/blob/main/sarsa_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "\n",
        "def grid_to_tuple(grid):\n",
        "    return tuple(tuple(row) for row in grid)\n",
        "\n",
        "def initialize_action_values(states, actions):\n",
        "    action_values = {}\n",
        "    for row in range(len(states)):\n",
        "        for col in range(len(states[0])):\n",
        "            states_dup = copy.deepcopy(states)\n",
        "            states_dup[row][col] = 1\n",
        "            state_tuple = grid_to_tuple(states_dup)\n",
        "            for action in actions:\n",
        "                state_action_pair = (state_tuple, action)\n",
        "                action_values[state_action_pair] = 0.0\n",
        "    return action_values\n",
        "\n",
        "def epsilon_greedy_policy(state_tuple, actions, action_values, epsilon=0.1):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        q_values = [(action, action_values.get((state_tuple, action), 0.0)) for action in actions]\n",
        "        max_value = max(q_values, key=lambda x: x[1])[1]\n",
        "        best_actions = [action for action, value in q_values if value == max_value]\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "def is_terminal_state(state):\n",
        "    destination_row = 3\n",
        "    destination_col = 7\n",
        "    if (state[destination_row][destination_col] == 1):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_reward(cur_row, cur_col):\n",
        "    goal_row = 3\n",
        "    goal_col = 7\n",
        "    dist = abs(cur_row - goal_row) + abs(cur_col - goal_col)\n",
        "    if cur_row == goal_row and cur_col == goal_col:\n",
        "        return 1.0\n",
        "    return -dist * 0.01 - 0.01\n",
        "\n",
        "\n",
        "def get_next_coordinates(cur_row, cur_col, action, max_rows=7, max_cols=10):\n",
        "    if action == 0:  # left\n",
        "        next_row = cur_row\n",
        "        next_col = max(0, cur_col - 1)\n",
        "    elif action == 1:  # down\n",
        "        next_row = min(max_rows - 1, cur_row + 1)\n",
        "        next_col = cur_col\n",
        "    elif action == 2:  # right\n",
        "        next_row = cur_row\n",
        "        next_col = min(max_cols - 1, cur_col + 1)\n",
        "    elif action == 3:  # up\n",
        "        next_row = max(0, cur_row - 1)\n",
        "        next_col = cur_col\n",
        "    elif action == 4:  # down-left\n",
        "        next_row = min(max_rows - 1, cur_row + 1)\n",
        "        next_col = max(0, cur_col - 1)\n",
        "    elif action == 5:  # up-left\n",
        "        next_row = max(0, cur_row - 1)\n",
        "        next_col = max(0, cur_col - 1)\n",
        "    elif action == 6:  # up-right\n",
        "        next_row = max(0, cur_row - 1)\n",
        "        next_col = min(max_cols - 1, cur_col + 1)\n",
        "    elif action == 7:  # down-right\n",
        "        next_row = min(max_rows - 1, cur_row + 1)\n",
        "        next_col = min(max_cols - 1, cur_col + 1)\n",
        "    else:\n",
        "        next_row, next_col = cur_row, cur_col\n",
        "    return next_row, next_col\n",
        "\n",
        "\n",
        "def get_after_wind_coordinates(row_expected, col_expected, grid):\n",
        "    max_rows = len(grid)\n",
        "    max_cols = len(grid[0])\n",
        "    if 0 <= row_expected < max_rows and 0 <= col_expected < max_cols:\n",
        "        wind_strength = grid[row_expected][col_expected]\n",
        "        row_actual = max(0, row_expected - wind_strength)\n",
        "    else:\n",
        "        row_actual = row_expected\n",
        "    col_actual = col_expected\n",
        "    return row_actual, col_actual\n",
        "\n",
        "\n",
        "def sarsa_algo(states, actions, action_values, source_row, source_col):\n",
        "    grid_world = []\n",
        "    for i in range(7):\n",
        "        grid_world.append([0, 0, 0, 1, 1, 1, 2, 2, 1, 0])\n",
        "    epsilon = 0.1\n",
        "\n",
        "    for episode in range(10000):\n",
        "        epsilon = max(0.01, epsilon * 0.9995)\n",
        "        state_episode = copy.deepcopy(states)\n",
        "        state_episode[source_row][source_col] = 1\n",
        "        state_tuple = grid_to_tuple(state_episode)\n",
        "        action = epsilon_greedy_policy(state_tuple, actions, action_values, epsilon)\n",
        "        steps_per_episode = 0\n",
        "        cur_row = source_row\n",
        "        cur_col = source_col\n",
        "        max_steps = 1000\n",
        "\n",
        "        while(not is_terminal_state(state_episode) and steps_per_episode < max_steps):\n",
        "            steps_per_episode += 1\n",
        "            next_row_expected, next_col_expected = get_next_coordinates(cur_row, cur_col, action)\n",
        "            next_row_actual, next_col_actual = get_after_wind_coordinates(next_row_expected, next_col_expected, grid_world)\n",
        "            next_state = copy.deepcopy(states)\n",
        "            next_state[next_row_actual][next_col_actual] = 1\n",
        "            immediate_reward = get_reward(next_row_actual, next_col_actual)\n",
        "            next_state_tuple = grid_to_tuple(next_state)\n",
        "            next_action = epsilon_greedy_policy(next_state_tuple, actions, action_values, epsilon)\n",
        "            next_state_action_pair = (next_state_tuple, next_action)\n",
        "\n",
        "            if (is_terminal_state(next_state)):\n",
        "                action_values[(state_tuple, action)] = action_values[(state_tuple, action)] + 0.2 * (immediate_reward - action_values[(state_tuple, action)])\n",
        "                if episode % 100 == 0:\n",
        "                  print(f\"Episode {episode}: Steps {steps_per_episode+1}\")\n",
        "                break\n",
        "\n",
        "            action_values[(state_tuple, action)] = action_values[(state_tuple, action)] + 0.2 * (immediate_reward + (0.9 * action_values[next_state_action_pair]) - action_values[(state_tuple, action)])\n",
        "            state_episode = next_state\n",
        "            state_tuple = next_state_tuple\n",
        "            action = next_action\n",
        "            cur_row = next_row_actual\n",
        "            cur_col = next_col_actual\n",
        "        if steps_per_episode >= max_steps:\n",
        "            print(f\"Episode {episode}: Reached maximum steps without finding goal\")\n",
        "\n",
        "\n",
        "def print_policy(action_values, states, actions):\n",
        "    empty_states = [[0 for _ in range(len(states[0]))] for _ in range(len(states))]\n",
        "    direction_symbols = {\n",
        "        0: \"←\",\n",
        "        1: \"↓\",\n",
        "        2: \"→\",\n",
        "        3: \"↑\",\n",
        "        4: \"↙\",\n",
        "        5: \"↖\",\n",
        "        6: \"↗\",\n",
        "        7: \"↘\",\n",
        "    }\n",
        "    directions = [[0 for _ in range(10)] for _ in range(7)]\n",
        "\n",
        "    for i in range(7):\n",
        "        for j in range(10):\n",
        "            empty_states[i][j] = 1\n",
        "            state_tuple = grid_to_tuple(empty_states)\n",
        "\n",
        "            best_value_function = float('-inf')\n",
        "            best_action = -1\n",
        "            for a in actions:\n",
        "                state_value_function = action_values.get((state_tuple, a), 0)\n",
        "                if state_value_function > best_value_function:\n",
        "                    best_action = a\n",
        "                    best_value_function = state_value_function\n",
        "            directions[i][j] = best_action\n",
        "            empty_states[i][j] = 0\n",
        "\n",
        "    policy_symbols = [[direction_symbols.get(directions[i][j], \"·\") for j in range(10)] for i in range(7)]\n",
        "\n",
        "    print(\"\\nLearned Policy:\")\n",
        "    for i, row in enumerate(policy_symbols):\n",
        "        row_str = \"\"\n",
        "        for j, symbol in enumerate(row):\n",
        "            if i == 3 and j == 7:\n",
        "                row_str += \"G  \"\n",
        "            else:\n",
        "                row_str += symbol + \"  \"\n",
        "        print(row_str)\n",
        "\n",
        "grid_world = []\n",
        "for i in range(7):\n",
        "    grid_world.append([0, 0, 0, 1, 1, 1, 2, 2, 1, 0])\n",
        "\n",
        "states = [[0]*10 for _ in range(7)]\n",
        "actions = [0, 1, 2, 3, 4, 5, 6, 7]  # 0-left, 1-down, 2-right, 3-up, 4-down-left, 5-up-left, 6-up-right, 7-down-right\n",
        "\n",
        "action_values = initialize_action_values(states, actions)\n",
        "sarsa_algo(states, actions, action_values, 3, 0)\n",
        "print_policy(action_values, states, actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPxyBlkcShXr",
        "outputId": "3fd05eac-77fc-4d52-8afe-7ef8ed87c972"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Reached maximum steps without finding goal\n",
            "Episode 100: Steps 21\n",
            "Episode 200: Steps 16\n",
            "Episode 300: Steps 17\n",
            "Episode 400: Steps 16\n",
            "Episode 500: Steps 18\n",
            "Episode 600: Steps 15\n",
            "Episode 700: Steps 17\n",
            "Episode 800: Steps 15\n",
            "Episode 900: Steps 16\n",
            "Episode 1000: Steps 15\n",
            "Episode 1100: Steps 15\n",
            "Episode 1200: Steps 15\n",
            "Episode 1300: Steps 15\n",
            "Episode 1400: Steps 15\n",
            "Episode 1500: Steps 17\n",
            "Episode 1600: Steps 15\n",
            "Episode 1700: Steps 18\n",
            "Episode 1800: Steps 18\n",
            "Episode 1900: Steps 15\n",
            "Episode 2000: Steps 15\n",
            "Episode 2100: Steps 15\n",
            "Episode 2200: Steps 16\n",
            "Episode 2300: Steps 15\n",
            "Episode 2400: Steps 15\n",
            "Episode 2500: Steps 15\n",
            "Episode 2600: Steps 15\n",
            "Episode 2700: Steps 15\n",
            "Episode 2800: Steps 15\n",
            "Episode 2900: Steps 15\n",
            "Episode 3000: Steps 15\n",
            "Episode 3100: Steps 15\n",
            "Episode 3200: Steps 17\n",
            "Episode 3300: Steps 15\n",
            "Episode 3400: Steps 15\n",
            "Episode 3500: Steps 15\n",
            "Episode 3600: Steps 15\n",
            "Episode 3700: Steps 15\n",
            "Episode 3800: Steps 15\n",
            "Episode 3900: Steps 15\n",
            "Episode 4000: Steps 15\n",
            "Episode 4100: Steps 15\n",
            "Episode 4200: Steps 16\n",
            "Episode 4300: Steps 15\n",
            "Episode 4400: Steps 15\n",
            "Episode 4500: Steps 15\n",
            "Episode 4600: Steps 17\n",
            "Episode 4700: Steps 15\n",
            "Episode 4800: Steps 15\n",
            "Episode 4900: Steps 17\n",
            "Episode 5000: Steps 15\n",
            "Episode 5100: Steps 18\n",
            "Episode 5200: Steps 15\n",
            "Episode 5300: Steps 15\n",
            "Episode 5400: Steps 15\n",
            "Episode 5500: Steps 15\n",
            "Episode 5600: Steps 15\n",
            "Episode 5700: Steps 15\n",
            "Episode 5800: Steps 15\n",
            "Episode 5900: Steps 15\n",
            "Episode 6000: Steps 15\n",
            "Episode 6100: Steps 15\n",
            "Episode 6200: Steps 15\n",
            "Episode 6300: Steps 23\n",
            "Episode 6400: Steps 15\n",
            "Episode 6500: Steps 15\n",
            "Episode 6600: Steps 15\n",
            "Episode 6700: Steps 15\n",
            "Episode 6800: Steps 15\n",
            "Episode 6900: Steps 15\n",
            "Episode 7000: Steps 15\n",
            "Episode 7100: Steps 15\n",
            "Episode 7200: Steps 16\n",
            "Episode 7300: Steps 15\n",
            "Episode 7400: Steps 15\n",
            "Episode 7500: Steps 17\n",
            "Episode 7600: Steps 15\n",
            "Episode 7700: Steps 15\n",
            "Episode 7800: Steps 15\n",
            "Episode 7900: Steps 15\n",
            "Episode 8000: Steps 17\n",
            "Episode 8100: Steps 15\n",
            "Episode 8200: Steps 15\n",
            "Episode 8300: Steps 15\n",
            "Episode 8400: Steps 15\n",
            "Episode 8500: Steps 15\n",
            "Episode 8600: Steps 15\n",
            "Episode 8700: Steps 15\n",
            "Episode 8800: Steps 15\n",
            "Episode 8900: Steps 15\n",
            "Episode 9000: Steps 15\n",
            "Episode 9100: Steps 15\n",
            "Episode 9200: Steps 18\n",
            "Episode 9300: Steps 15\n",
            "Episode 9400: Steps 15\n",
            "Episode 9500: Steps 15\n",
            "Episode 9600: Steps 15\n",
            "Episode 9700: Steps 15\n",
            "Episode 9800: Steps 15\n",
            "Episode 9900: Steps 15\n",
            "\n",
            "Learned Policy:\n",
            "↘  ↓  ↓  ↗  →  ↗  →  ↘  ↘  ↓  \n",
            "↘  ↘  ↘  ↘  ↘  ↘  ↓  ↓  ↘  ↓  \n",
            "→  ↘  →  ↘  ↘  ↘  →  →  ↘  ↘  \n",
            "→  →  ↘  ↘  ↘  ↘  ↗  G  ↘  ↘  \n",
            "→  ↘  →  →  →  →  ←  ←  ↙  ↙  \n",
            "→  ↗  ↗  ↑  ↓  ←  ←  ←  ←  ←  \n",
            "↗  ↗  ↗  ←  ←  ←  ←  ←  ←  ↖  \n"
          ]
        }
      ]
    }
  ]
}